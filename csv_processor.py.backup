"""
CSV Processor Module for Email Scraper & Validator
Handles parallel processing of CSV files with URL scraping and contact extraction.
"""

import pandas as pd
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Optional, Tuple, Any
import time
from tqdm import tqdm
import os
from urllib.parse import urlparse
import csv
from collections import deque
import threading
import queue

from web_scraper import WebScraper
from contact_extractor import ContactExtractor
from email_validation import EmailValidator


class CSVProcessor:
    """
    Handles parallel processing of CSV files containing URLs for contact extraction.
    """

    def __init__(self, max_workers: int = 10, timeout: int = 30, block_images: bool = False, disable_resources: bool = False, network_idle: bool = True, cf_wait_timeout: int = 60, skip_on_challenge: bool = False, proxy_file: str = "proxy.txt"):
        """
        Initialize CSV processor.

        Args:
            max_workers: Maximum number of concurrent threads
            timeout: Timeout for web scraping requests
            block_images: Block image loading to reduce bandwidth
            disable_resources: Disable fonts/media/other non-essential resources
            network_idle: Wait for network idle state during dynamic fetch
            cf_wait_timeout: Per-URL maximum wait for Cloudflare challenge (seconds)
            skip_on_challenge: Skip immediately when Cloudflare challenge detected
            proxy_file: Path to proxy file for automatic proxy detection
        """
        self.max_workers = max_workers
        self.timeout = timeout
        self.scraper = WebScraper(timeout=timeout, block_images=block_images, disable_resources=disable_resources, network_idle=network_idle, cf_wait_timeout=cf_wait_timeout, skip_on_challenge=skip_on_challenge, proxy_file=proxy_file)
        self.extractor = ContactExtractor()
        self.validator = EmailValidator()

        # Setup module logger - rely on root configuration
        self.logger = logging.getLogger(__name__)

    def process_single_url(self, url_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process a single URL and extract contact information.

        Args:
            url_data: Dictionary containing URL and metadata

        Returns:
            Dictionary with extracted contact information
        """
        # Handle NaN values from pandas
        raw_url = url_data.get('url', '')
        if pd.isna(raw_url):
            url = ''
        else:
            url = str(raw_url).strip()
        row_index = url_data.get('index', 0)

        result = {
            'index': row_index,
            'url': url,
            'status': 'failed',
            'emails': [],
            'phones': [],
            'whatsapp': [],
            'validated_emails': {},
            'error': None,
            'processing_time': 0
        }
        # Preserve original row data for streaming CSV writing
        result['original_data'] = url_data.get('original_data', {})

        start_time = time.time()

        try:
            if not url or not self._is_valid_url(url):
                result['error'] = 'Invalid URL format'
                return result

            # Skip social media URLs (Facebook, Instagram, LinkedIn, etc.)
            if self._is_social_url(url):
                result['status'] = 'skipped_social'
                result['error'] = 'Social media URL skipped'
                return result

            # Use the new gather_contact_info method for comprehensive scraping
            scrape_result = self.scraper.gather_contact_info(url)

            # Check for errors
            if scrape_result.get('error'):
                result['error'] = scrape_result['error']
                return result

            # Check if we got any content
            if scrape_result.get('status') == 'no_contacts_found':
                result['status'] = 'no_contacts_found'
                result['error'] = 'No contact information found on the website'
                return result

            # Extract contact information directly from scrape_result
            result['emails'] = scrape_result.get('emails', [])
            result['phones'] = scrape_result.get('phones', [])
            result['whatsapp'] = scrape_result.get('whatsapp', [])
            result['pages_scraped'] = len(scrape_result.get('pages_scraped', []))

            # Validation moved to dedicated SMTP pool (consumer stage)

            # Set success status
            if result['emails'] or result['phones'] or result['whatsapp']:
                result['status'] = 'success'
            else:
                result['status'] = 'no_contacts_found'

        except Exception as e:
            self.logger.error(f"Error processing URL {url}: {str(e)}")
            result['error'] = str(e)

        finally:
            result['processing_time'] = time.time() - start_time

        return result

    def process_csv_file(self,
                        input_file: str,
                        output_file: str,
                        url_column: str = 'url',
                        batch_size: int = 100,
                        input_chunksize: int = 0) -> Dict[str, Any]:
        """
        Process a CSV file with URLs in parallel.

        Args:
            input_file: Path to input CSV file
            output_file: Path to output CSV file
            url_column: Name of the column containing URLs
            batch_size: Number of URLs to process in each batch

        Returns:
            Dictionary with processing statistics
        """
        try:
            # Read header only to validate columns without loading full data
            columns_df = pd.read_csv(input_file, nrows=0)
            if url_column not in columns_df.columns:
                raise ValueError(f"Column '{url_column}' not found in CSV file")

            # Determine total rows with non-empty URL for progress bar
            total_urls = 0
            with open(input_file, 'r', encoding='utf-8', newline='') as f_in:
                reader = csv.DictReader(f_in)
                for r in reader:
                    u = (r.get(url_column) or '').strip()
                    if u:
                        total_urls += 1
            self.logger.info(f"ðŸ“Š Found {total_urls} URLs to process")

            # Prepare streaming CSV writer
            os.makedirs(os.path.dirname(output_file) or '.', exist_ok=True)

            # Build header order: original cols (drop 'email' if present) + contacts + metrics
            original_cols = [c for c in columns_df.columns if c != 'email']
            contact_cols = ['emails', 'phones', 'whatsapp', 'validated_emails']
            metrics_cols = [
                'scraping_status', 'scraping_error', 'processing_time', 'pages_scraped',
                'emails_found', 'phones_found', 'whatsapp_found', 'validated_emails_count'
            ]
            header = original_cols + contact_cols + metrics_cols

            # Open output file and write header
            csv_file = open(output_file, 'w', newline='', encoding='utf-8')
            writer = csv.DictWriter(csv_file, fieldnames=header)
            writer.writeheader()

            # Stats counters
            processed_count = 0
            success_count = 0
            total_emails = 0
            total_validated_emails = 0
            total_phones = 0
            total_whatsapp = 0
            total_processing_time = 0.0

            # Rolling throughput window (10 minutes) for "processing per menit"
            window_secs = 600
            completion_times = deque()
            last_log_time = time.time()
            per_menit = 0.0

            # Create progress bar
            with tqdm(total=total_urls, desc="Processing URLs") as pbar:
                # Locks for thread-safe writer and progress updates
                writer_lock = threading.Lock()
                progress_lock = threading.Lock()

                # Tentukan ukuran pool SMTP validation
                if self.max_workers < 10:
                    smtp_workers = 10
                else:
                    smtp_workers = max(1, int(round(self.max_workers * 0.6)))
                # Simplified logging - no need for SMTP pool details
                pass

                # Shared queue between producer (scraping) and consumer (SMTP validation)
                scraped_queue: "queue.Queue[Dict[str, Any] | None]" = queue.Queue(maxsize=batch_size * 2)

                def consumer_loop():
                    nonlocal processed_count, success_count, total_emails, total_validated_emails, total_phones, total_whatsapp, total_processing_time, per_menit, completion_times, last_log_time
                    while True:
                        item = scraped_queue.get()
                        if item is None:
                            scraped_queue.task_done()
                            break

                        result = item

                        # Perform SMTP validation in consumer stage
                        validated_emails = {}
                        emails_list = result.get('emails', []) or []
                        if emails_list:
                            try:
                                validated_emails = self.validator.validate_batch(emails_list)
                            except Exception as e:
                                self.logger.error(f"Validation error for URL {result.get('url','')}: {str(e)}")
                                validated_emails = {}
                        result['validated_emails'] = validated_emails

                        # Update stats
                        emails_count = len(emails_list)
                        phones_count = len(result.get('phones', []) or [])
                        whatsapp_count = len(result.get('whatsapp', []) or [])
                        validated_count = len(validated_emails)

                        total_emails += emails_count
                        total_phones += phones_count
                        total_whatsapp += whatsapp_count
                        total_validated_emails += validated_count
                        total_processing_time += float(result.get('processing_time', 0) or 0)

                        # Prepare CSV row
                        original_row = result.get('original_data', {}) or {}
                        if 'email' in original_row:
                            original_row.pop('email', None)

                        output_row = {
                            **original_row,
                            'scraping_status': result['status'],
                            'scraping_error': result.get('error', ''),
                            'processing_time': result.get('processing_time', 0),
                            'pages_scraped': result.get('pages_scraped', 0),
                            'emails_found': emails_count,
                            'phones_found': phones_count,
                            'whatsapp_found': whatsapp_count,
                            'validated_emails_count': validated_count,
                            'emails': '; '.join(emails_list),
                            'phones': '; '.join(result.get('phones', []) or []),
                            'whatsapp': '; '.join(result.get('whatsapp', []) or []),
                            'validated_emails': '; '.join([
                                f"{email} ({validation_result.get('reason', 'unknown')})"
                                for email, validation_result in validated_emails.items()
                            ])
                        }

                        for key in header:
                            if key not in output_row:
                                output_row[key] = ''

                        # Write CSV row
                        with writer_lock:
                            writer.writerow(output_row)

                        # Update counters and progress
                        with progress_lock:
                            if result['status'] == 'success':
                                success_count += 1

                            processed_count += 1
                            pbar.update(1)

                            now_ts = time.time()
                            completion_times.append(now_ts)
                            while completion_times and (now_ts - completion_times[0]) > window_secs:
                                completion_times.popleft()
                            per_menit = len(completion_times) / 10.0

                            pbar.set_description(
                                f"Processing URLs (Success: {success_count}/{processed_count})"
                            )
                            pbar.set_postfix({"processing_per_menit": f"{per_menit:.2f}"})

                            # Removed frequent logging - only show in progress bar
                            pass

                        scraped_queue.task_done()

                # Start consumer pool
                consumer_executor = ThreadPoolExecutor(max_workers=smtp_workers)
                for _ in range(smtp_workers):
                    consumer_executor.submit(consumer_loop)

                # Start producer pool
                producer_executor = ThreadPoolExecutor(max_workers=self.max_workers)

                try:
                    # Build chunk iterator (streaming if input_chunksize > 0)
                    if input_chunksize and input_chunksize > 0:
                        chunk_iter = pd.read_csv(input_file, chunksize=input_chunksize)
                    else:
                        chunk_iter = [pd.read_csv(input_file)]

                    # Process each chunk
                    for df_chunk in chunk_iter:
                        # Prepare URL data for this chunk
                        url_data_list = []
                        for index, row in df_chunk.iterrows():
                            url_data = {
                                'index': index,
                                'url': row.get(url_column),
                                'original_data': row.to_dict()
                            }
                            url_data_list.append(url_data)

                        # Submit tasks in batches to control memory
                        for i in range(0, len(url_data_list), batch_size):
                            batch = url_data_list[i:i + batch_size]

                            future_to_url = {
                                producer_executor.submit(self.process_single_url, url_data): url_data
                                for url_data in batch
                            }

                            # As producers finish, push results to consumer queue
                            for future in as_completed(future_to_url):
                                try:
                                    result = future.result()
                                except Exception as e:
                                    # Create minimal error result to keep pipeline flowing
                                    bad = future_to_url.get(future, {})
                                    result = {
                                        'index': bad.get('index', 0),
                                        'url': str(bad.get('url', '')),
                                        'status': 'error',
                                        'emails': [],
                                        'phones': [],
                                        'whatsapp': [],
                                        'validated_emails': {},
                                        'error': str(e),
                                        'processing_time': 0,
                                        'pages_scraped': 0,
                                        'original_data': bad.get('original_data', {})
                                    }
                                    self.logger.error(f"Error in producer result: {str(e)}")

                                # Push to queue for consumer validation and writing
                                scraped_queue.put(result)

                    # All producers submitted and completed; shut down producer executor
                    producer_executor.shutdown(wait=True)

                    # Signal consumers to exit by sending sentinel values
                    for _ in range(smtp_workers):
                        scraped_queue.put(None)

                    # Wait until all queued items are processed
                    scraped_queue.join()

                    # Shutdown consumer pool
                    consumer_executor.shutdown(wait=True)

                finally:
                    try:
                        producer_executor.shutdown(wait=False)
                    except Exception:
                        pass
                    try:
                        consumer_executor.shutdown(wait=False)
                    except Exception:
                        pass

            # Close CSV file after streaming writes
            try:
                csv_file.close()
            except Exception:
                pass

            # Calculate statistics
            stats = {
                'total_urls': total_urls,
                'processed': processed_count,
                'successful': success_count,
                'failed': processed_count - success_count,
                'success_rate': (success_count / processed_count * 100) if processed_count > 0 else 0,
                'total_emails': total_emails,
                'total_validated_emails': total_validated_emails,
                'total_phones': total_phones,
                'total_whatsapp': total_whatsapp,
                'average_processing_time': (total_processing_time / processed_count) if processed_count > 0 else 0,
                'processing_per_menit': per_menit
            }

            self.logger.info(f"âœ… Completed! Success rate: {stats['success_rate']:.1f}% ({success_count}/{processed_count})")
            return stats

        except Exception as e:
            self.logger.error(f"Error processing CSV file: {str(e)}")
            raise

    def _save_results_to_csv(self, results: List[Dict], original_df: pd.DataFrame, output_file: str):
        """
        Save processing results to CSV file.

        Args:
            results: List of processing results
            original_df: Original DataFrame
            output_file: Path to output CSV file
        """
        # Create output DataFrame
        output_data = []

        for result in results:
            row_index = result['index']
            original_row = original_df.iloc[row_index].to_dict()

            # Create base row with original data
            output_row = original_row.copy()

            # Add processing results
            output_row.update({
                'scraping_status': result['status'],
                'scraping_error': result.get('error', ''),
                'processing_time': result.get('processing_time', 0),
                'pages_scraped': result.get('pages_scraped', 0),
                'emails_found': len(result.get('emails', [])),
                'phones_found': len(result.get('phones', [])),
                'whatsapp_found': len(result.get('whatsapp', [])),
                'validated_emails_count': len(result.get('validated_emails', {})),
                'emails': '; '.join(result.get('emails', [])),
                'phones': '; '.join(result.get('phones', [])),
                'whatsapp': '; '.join(result.get('whatsapp', [])),
                'validated_emails': '; '.join([
                    f"{email} ({validation_result.get('reason', 'unknown')})"
                    for email, validation_result in result.get('validated_emails', {}).items()
                ])
            })

            output_data.append(output_row)

        # Create DataFrame and save
        output_df = pd.DataFrame(output_data)

        # Remove duplicate/sumber kolom 'email' dari input jika ada,
        # agar hanya satu kolom email yang digunakan ('emails')
        if 'email' in output_df.columns and 'emails' in output_df.columns:
            try:
                output_df = output_df.drop(columns=['email'])
            except Exception:
                pass

        # Reorder columns: taruh metrik di paling belakang setelah 'validated_emails'
        try:
            original_cols = list(original_df.columns)
            # pastikan mengabaikan kolom 'email' jika sudah dihapus
            original_cols = [c for c in original_cols if c != 'email' and c in output_df.columns]

            contact_cols = [c for c in ['emails', 'phones', 'whatsapp', 'validated_emails'] if c in output_df.columns]
            metrics_cols = [c for c in [
                'scraping_status', 'scraping_error', 'processing_time', 'pages_scraped',
                'emails_found', 'phones_found', 'whatsapp_found', 'validated_emails_count'
            ] if c in output_df.columns]

            # Susun urutan: original -> contacts -> metrics (metrics paling belakang)
            desired_order = original_cols + contact_cols + metrics_cols
            # Tambahkan kolom lain yang belum tercakup di posisi sebelum metrics
            remaining = [c for c in output_df.columns if c not in desired_order]
            # Tempatkan remaining sebelum metrics jika ada
            # temukan indeks mulai metrics dalam desired_order
            if metrics_cols:
                # sisipkan remaining tepat sebelum blok metrics
                insert_pos = len(original_cols + contact_cols)
                desired_order = desired_order[:insert_pos] + remaining + desired_order[insert_pos:]
            else:
                desired_order = desired_order + remaining

            output_df = output_df[desired_order]
        except Exception:
            # Jika ada masalah, tetap lanjut tanpa mengubah urutan
            pass

        # Ensure output directory exists
        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        # Save to CSV
        output_df.to_csv(output_file, index=False)
        self.logger.info(f"Results saved to {output_file}")

    def _is_valid_url(self, url: str) -> bool:
        """
        Check if URL is valid.

        Args:
            url: URL to validate

        Returns:
            True if URL is valid, False otherwise
        """
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc])
        except Exception:
            return False

    def _is_social_url(self, url: str) -> bool:
        """Detect if a URL belongs to a social media domain and should be skipped."""
        try:
            parsed = urlparse(url)
            host = (parsed.netloc or '').lower()
            social_domains = [
                'facebook.com', 'instagram.com', 'twitter.com', 'x.com', 'linkedin.com',
                'youtube.com', 'tiktok.com', 'pinterest.com', 'vk.com', 'snapchat.com',
                'wa.me', 'web.whatsapp.com', 'discord.com'
            ]
            return any(host.endswith(d) or d in host for d in social_domains)
        except Exception:
            return False

    def process_multiple_csv_files(self,
                                  input_files: List[str],
                                  output_dir: str,
                                  url_column: str = 'url') -> Dict[str, Any]:
        """
        Process multiple CSV files.

        Args:
            input_files: List of input CSV file paths
            output_dir: Output directory for processed files
            url_column: Name of the column containing URLs

        Returns:
            Dictionary with overall processing statistics
        """
        os.makedirs(output_dir, exist_ok=True)

        overall_stats = {
            'files_processed': 0,
            'total_urls': 0,
            'total_successful': 0,
            'total_failed': 0,
            'file_results': []
        }

        for input_file in input_files:
            try:
                # Generate output filename
                base_name = os.path.splitext(os.path.basename(input_file))[0]
                output_file = os.path.join(output_dir, f"{base_name}_processed.csv")

                # Process file
                stats = self.process_csv_file(input_file, output_file, url_column)

                # Update overall statistics
                overall_stats['files_processed'] += 1
                overall_stats['total_urls'] += stats['total_urls']
                overall_stats['total_successful'] += stats['successful']
                overall_stats['total_failed'] += stats['failed']
                overall_stats['file_results'].append({
                    'input_file': input_file,
                    'output_file': output_file,
                    'stats': stats
                })

            except Exception as e:
                self.logger.error(f"Error processing file {input_file}: {str(e)}")
                overall_stats['file_results'].append({
                    'input_file': input_file,
                    'error': str(e)
                })

        # Calculate overall success rate
        if overall_stats['total_urls'] > 0:
            overall_stats['overall_success_rate'] = (
                overall_stats['total_successful'] / overall_stats['total_urls'] * 100
            )
        else:
            overall_stats['overall_success_rate'] = 0

        return overall_stats


if __name__ == "__main__":
    # Example usage
    processor = CSVProcessor(max_workers=5, timeout=30)

    # Process single CSV file
    stats = processor.process_csv_file(
        input_file="input_urls.csv",
        output_file="output_results.csv",
        url_column="website_url"
    )

    print(f"Processing completed with {stats['success_rate']:.1f}% success rate")
    print(f"Total emails found: {stats['total_emails']}")
    print(f"Total validated emails: {stats['total_validated_emails']}")