"""
Web Scraper Module
Handles web scraping with anti-bot bypass and JavaScript rendering using Scrapling.
"""

import time
import logging
import re
import sys
import multiprocessing
import queue
from types import SimpleNamespace
from typing import Dict, Optional, List
import os
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs
import json
from urllib.request import Request, urlopen
from urllib.error import URLError, HTTPError
import threading

# Import proxy manager
from proxy_manager import ProxyManager

# Thread-local context to carry current domain for Scrapling logs
_SCRAPLING_CONTEXT = threading.local()

# Subprocess fetch helper to hard-isolate StealthyFetcher logging and enforce wall-clock timeout
def _subprocess_fetch(q, url, headless, solve_cloudflare, network_idle, google_search, timeout, block_images, disable_resources, proxy_config=None):
    """
    Child process target: perform StealthyFetcher.fetch and return minimal page fields via Queue.
    Logging is globally disabled and stdout/stderr are wrapped to suppress Cloudflare progress spam.
    """
    try:
        # Disable all Python logging in child process
        try:
            logging.disable(logging.CRITICAL)
        except Exception:
            pass

        # Wrap stdout/stderr to suppress known Cloudflare progress lines
        try:
            subs = (
                'waiting for cloudflare wait page to disappear',
                'turnstile version discovered',
                'no cloudflare challenge found',
            )
            class _SuppressingStream:
                def __init__(self, wrapped, substrings):
                    self._wrapped = wrapped
                    self._subs = tuple(s.lower() for s in substrings)

                def write(self, s):
                    try:
                        low = str(s).lower()
                        if any(sub in low for sub in self._subs):
                            return len(s)
                        if re.search(r'waiting\s+for\s+cloudflare\s+wait\s+page\s+to\s+disappear', low):
                            return len(s)
                    except Exception:
                        pass
                    try:
                        return self._wrapped.write(s)
                    except Exception:
                        return 0

                def flush(self):
                    try:
                        return self._wrapped.flush()
                    except Exception:
                        return None

            sys.stdout = _SuppressingStream(sys.stdout, subs)
            sys.stderr = _SuppressingStream(sys.stderr, subs)
        except Exception:
            pass

        # Lightweight page_action to allow CF-critical resources while trimming heavy assets
        def _page_action(page):
            try:
                def handler(route):
                    req = route.request
                    u = (getattr(req, 'url', '') or '').lower()
                    rtype = (getattr(req, 'resource_type', '') or '').lower()

                    allow_domains = (
                        'cloudflare', 'cf-challenge', 'challenge', 'turnstile', 'captcha',
                        'cdnjs.cloudflare.com', 'cfassets', 'cfcdn', 'hcaptcha', 'recaptcha',
                        'cdn-cgi', 'cf-ray', 'cflare', 'data-cf'
                    )
                    if any(k in u for k in allow_domains):
                        return route.continue_()

                    essential_types = ('document', 'script', 'stylesheet', 'xhr', 'fetch')
                    if rtype in essential_types:
                        return route.continue_()

                    if block_images and rtype in ('image', 'imageset'):
                        return route.abort()

                    if disable_resources and rtype in ('font', 'media', 'video', 'audio'):
                        return route.abort()

                    # Fix: Block non-essential resources in light-load mode instead of allowing all
                    if block_images or disable_resources:
                        return route.abort()
                    return route.continue_()
                page.route("**/*", handler)
            except Exception:
                pass

        # Import Scrapling inside child to avoid initializing its loggers in the parent process
        from scrapling.fetchers import StealthyFetcher  # noqa: E402
        
        # Enhanced Cloudflare bypass configuration
        import os
        os.environ['PLAYWRIGHT_CF_AGGRESSIVE'] = '1'  # Enable aggressive mode

        # Reduce Playwright/Node driver logging noise and avoid debug output
        try:
            os.environ.setdefault('PLAYWRIGHT_DISABLE_LOG', '1')
            os.environ.setdefault('PWDEBUG', '0')
            # Disable Node debug logs commonly used by Playwright
            if 'DEBUG' in os.environ:
                # Keep user's DEBUG if set but mask playwright verbose categories
                if 'playwright' in str(os.environ.get('DEBUG', '')).lower():
                    os.environ['DEBUG'] = ''
        except Exception:
            pass

        # Prepare StealthyFetcher arguments
        fetch_kwargs = {
            'url': url,
            'headless': headless,
            'solve_cloudflare': solve_cloudflare,
            'network_idle': network_idle,
            'google_search': google_search,
            'timeout': timeout,
            'block_images': block_images,
            'disable_resources': disable_resources,
            'page_action': _page_action if (block_images or disable_resources) else None,
            'geoip': True  # Fix proxy warning - recommended when using proxies
        }
        
        # Add proxy configuration if provided
        if proxy_config:
            fetch_kwargs['proxy'] = proxy_config
        
        page = StealthyFetcher.fetch(**fetch_kwargs)

        q.put({
            'ok': True,
            'status': getattr(page, 'status', 200),
            'html_content': getattr(page, 'html_content', ''),
            'final_url': getattr(page, 'url', url),
        })
    except Exception as e:
        q.put({'ok': False, 'error': str(e)})


class WebScraper:
    """Web scraper with Cloudflare bypass and stealth capabilities."""

    def __init__(self,
                 headless: bool = True,
                 solve_cloudflare: bool = True,
                 timeout: int = 30,
                 network_idle: bool = True,
                 block_images: bool = False,
                 disable_resources: bool = False,
                 static_first: bool = True,
                 cf_wait_timeout: int = 60,
                 skip_on_challenge: bool = False,
                 proxy_file: str = "proxy.txt"):
        """
        Initialize the web scraper.

        Args:
            headless (bool): Run browser in background
            solve_cloudflare (bool): Enable Cloudflare bypass
            timeout (int): Timeout in seconds
            network_idle (bool): Wait for network idle state
            block_images (bool): Prevent image loading to save bandwidth
            disable_resources (bool): Drop non-essential resources (fonts, media, etc.)
            static_first (bool): Try static HTTP request first before browser automation
            cf_wait_timeout (int): Cloudflare challenge timeout
            skip_on_challenge (bool): Skip URLs with detected challenges
            proxy_file (str): Path to proxy file (default: proxy.txt)
        """
        self.headless = headless
        self.solve_cloudflare = solve_cloudflare
        self.timeout = timeout
        self.network_idle = network_idle
        self.block_images = block_images
        self.disable_resources = disable_resources
        self.static_first = static_first
        # Per-URL maximum wait for Cloudflare wait page before skipping
        self.cf_wait_timeout = cf_wait_timeout
        # If True, skip early when a Cloudflare challenge is detected
        self.skip_on_challenge = skip_on_challenge
        
        # Initialize proxy manager
        self.proxy_manager = ProxyManager(proxy_file)
        
        # Log proxy status
        if self.proxy_manager.has_proxies():
            self.logger = logging.getLogger(__name__)
            self.logger.info(f"ðŸ”„ Proxy support enabled: {self.proxy_manager.get_proxy_count()} proxies loaded")
        else:
            self.logger = logging.getLogger(__name__)
            self.logger.info("ðŸ“¡ No proxy file detected - running in direct mode")

        # Suppress harmless Cloudflare messages from Scrapling
        class _CloudflareLogFilter(logging.Filter):
            def filter(self, record: logging.LogRecord) -> bool:
                # Be tolerant to logging message formats
                try:
                    msg = record.getMessage()
                except Exception:
                    msg = str(getattr(record, 'msg', ''))
                low = str(msg).lower()

                # Case-insensitive substring suppression for common noisy lines
                suppressed_substrings = (
                    'no cloudflare challenge found',
                    'waiting for cloudflare wait page to disappear',  # handle punctuation via regex too
                    'the turnstile version discovered is',
                    'turnstile version discovered is',
                    'cloudflare wait page detected',
                    'solving cloudflare challenge',
                )
                if any(s in low for s in suppressed_substrings):
                    return False

                # Regex suppression to capture punctuation/format variations
                patterns = (
                    r'waiting\s+for\s+cloudflare\s+wait\s+page\s+to\s+disappear\.?',  # optional trailing period
                    r'turnstile\s+version\s+discovered\s+is\b',
                    r'cloudflare\s+(challenge|wait\s+page)\s+(detected|found)\b',
                )
                for p in patterns:
                    try:
                        if re.search(p, low):
                            return False
                    except Exception:
                        # Never block logging due to regex errors
                        pass

                return True

        # Prefix Scrapling messages with current domain when available
        class _DomainPrefixFilter(logging.Filter):
            def filter(self, record: logging.LogRecord) -> bool:
                try:
                    domain = getattr(_SCRAPLING_CONTEXT, 'domain', None)
                    if domain:
                        msg = record.getMessage()
                        # Avoid double-prefixing
                        if not str(msg).startswith(str(domain)):
                            record.msg = f"{domain} - {msg}"
                            record.args = None
                except Exception:
                    # Be tolerant: never block logging due to prefix issues
                    pass
                return True

        # Attach filters to likely Scrapling logger namespaces to ensure coverage
        # Aggressively suppress Scrapling CF progress logs:
        # - Disable scrapling loggers and remove their handlers
        # - Keep our domain prefix filter available for any remaining messages
        for name in ('scrapling', 'scrapling.fetchers', 'scrapling.playwright', 'scrapling.cloudflare', 'scrapling.turnstile'):
            lg = logging.getLogger(name)
            try:
                # Prevent INFO spam by gating at CRITICAL and disabling propagation
                lg.setLevel(logging.CRITICAL)
                lg.propagate = False

                # Remove existing handlers that might bypass logger-level gates
                for h in list(getattr(lg, 'handlers', [])):
                    try:
                        lg.removeHandler(h)
                    except Exception:
                        pass

                # Add a NullHandler to swallow any stray emits
                lg.addHandler(logging.NullHandler())

                # Hard disable the logger to ensure no records are processed
                lg.disabled = True
            except Exception:
                # Always be tolerant to logging setup errors
                pass

            # Still attach filters; in case the library flips disabled/handlers internally later
            lg.addFilter(_CloudflareLogFilter())
            lg.addFilter(_DomainPrefixFilter())

        # Enumerate ALL registered loggers and silence any 'scrapling*' namespaces,
        # including deep modules like 'scrapling.engines._browsers._camoufox'
        try:
            for lname, lobj in logging.Logger.manager.loggerDict.items():
                if not isinstance(lobj, logging.Logger):
                    continue
                if str(lname).startswith('scrapling'):
                    try:
                        lobj.setLevel(logging.CRITICAL)
                        lobj.propagate = False
                        # Remove and replace handlers to prevent pre-bound streams from bypassing filters
                        for h in list(getattr(lobj, 'handlers', [])):
                            try:
                                lobj.removeHandler(h)
                            except Exception:
                                pass
                        lobj.addHandler(logging.NullHandler())
                        lobj.disabled = True
                        # Attach filters defensively
                        lobj.addFilter(_CloudflareLogFilter())
                        lobj.addFilter(_DomainPrefixFilter())
                    except Exception:
                        # Suppression must never crash initialization
                        pass
        except Exception:
            # Be tolerant to logger manager issues
            pass

        # Additionally, install a global filter on the root logger and its handlers
        # to catch any stray records emitted outside our targeted logger namespaces.
        try:
            root = logging.getLogger()
            root.addFilter(_CloudflareLogFilter())
            for h in getattr(root, 'handlers', []):
                try:
                    h.addFilter(_CloudflareLogFilter())
                except Exception:
                    pass
        except Exception:
            # Be tolerant: logging setup should never crash the scraper
            pass

        # Final guardrail: wrap stdout/stderr to suppress print-based progress lines
        # from third-party libraries that bypass logging. This only filters known CF spam lines.
        class _SuppressingStream:
            def __init__(self, wrapped, substrings):
                self._wrapped = wrapped
                self._subs = tuple(s.lower() for s in substrings)
                self._lock = threading.Lock()

            def write(self, s):
                try:
                    low = str(s).lower()
                    # Drop lines containing known CF progress messages
                    if any(sub in low for sub in self._subs):
                        return len(s)
                    # Regex catch-all for phrasing variations
                    if re.search(r'waiting\s+for\s+cloudflare\s+wait\s+page\s+to\s+disappear', low):
                        return len(s)
                except Exception:
                    # Never break output on filter errors
                    pass
                with self._lock:
                    try:
                        return self._wrapped.write(s)
                    except Exception:
                        return 0

            def flush(self):
                try:
                    return self._wrapped.flush()
                except Exception:
                    return None

            def isatty(self):
                try:
                    return self._wrapped.isatty()
                except Exception:
                    return False

        try:
            _subs = (
                'waiting for cloudflare wait page to disappear',
                'turnstile version discovered',
                'no cloudflare challenge found',
            )
            # Wrap streams
            new_stdout = _SuppressingStream(sys.stdout, _subs)
            new_stderr = _SuppressingStream(sys.stderr, _subs)
            sys.stdout = new_stdout
            sys.stderr = new_stderr

            # Retarget existing StreamHandlers to the newly wrapped streams and add suppression filters
            try:
                # All named loggers
                for lname, lobj in logging.Logger.manager.loggerDict.items():
                    if not isinstance(lobj, logging.Logger):
                        continue
                    for h in list(getattr(lobj, 'handlers', [])):
                        try:
                            # Add suppression filter to handlers
                            h.addFilter(_CloudflareLogFilter())
                        except Exception:
                            pass
                        try:
                            # Force handler to use our wrapped stderr to intercept pre-bound streams
                            if getattr(h, 'stream', None) is not None:
                                h.stream = sys.stderr
                        except Exception:
                            pass

                # Root logger handlers
                root = logging.getLogger()
                for h in list(getattr(root, 'handlers', [])):
                    try:
                        h.addFilter(_CloudflareLogFilter())
                    except Exception:
                        pass
                    try:
                        if getattr(h, 'stream', None) is not None:
                            h.stream = sys.stderr
                    except Exception:
                        pass
            except Exception:
                # Defensive: handler retargeting should never break initialization
                pass
        except Exception:
            # Be tolerant: stdout/stderr wrapping should not crash the scraper
            pass

    def _build_allowlist_page_action(self):
        """Build a page_action that keeps Cloudflare/Turnstile-critical resources while blocking heavy assets.

        - Allows essential types: document, script, stylesheet, xhr/fetch
        - Always allows URLs containing Cloudflare/Turnstile/captcha indicators
        - Blocks images when block_images=True
        - Blocks fonts/media/video/audio when disable_resources=True
        """

        def _page_action(page):
            try:
                def handler(route):
                    req = route.request
                    url = (getattr(req, 'url', '') or '').lower()
                    rtype = (getattr(req, 'resource_type', '') or '').lower()

                    # Always allow Cloudflare/Turnstile/captcha-related resources
                    allow_domains = (
                        'cloudflare', 'cf-challenge', 'challenge', 'turnstile', 'captcha',
                        'cdnjs.cloudflare.com', 'cfassets', 'cfcdn', 'hcaptcha', 'recaptcha',
                        'cdn-cgi', 'cf-ray', 'cflare', 'data-cf'
                    )
                    if any(k in url for k in allow_domains):
                        return route.continue_()

                    # Allow essential resource types for page operation
                    essential_types = ('document', 'script', 'stylesheet', 'xhr', 'fetch')
                    if rtype in essential_types:
                        return route.continue_()

                    # Block heavy assets when requested
                    if self.block_images and rtype in ('image', 'imageset'):
                        return route.abort()

                    if self.disable_resources and rtype in ('font', 'media', 'video', 'audio'):
                        return route.abort()

                    # Default: allow
                    return route.continue_()

                # Install global route handler
                page.route("**/*", handler)
            except Exception:
                # Be tolerant: if routing fails, proceed without page_action
                pass

        return _page_action

    def _looks_like_challenge(self, html: str, url: str) -> bool:
        """Enhanced heuristic to detect ACTUAL Cloudflare challenge pages, not just CF presence."""
        if not html:
            return True
        
        # If HTML is substantial (>10KB), likely real content even with CF traces
        if len(html) > 10000:
            return False
            
        low = html.lower()
        
        # Strong challenge indicators (active challenge state)
        strong_indicators = (
            'just a moment', 'verify you are human', 'checking your browser',
            'please wait while', 'attention required', 'cf-please-wait',
            'redirectingâ€¦', 'redirecting...', 'ddos protection by',
            'browser verification', 'security check', 'loading...', 'challenge-platform'
        )
        
        # Weak indicators (CF presence but could be normal site)
        weak_indicators = (
            'cf-challenge', 'data-cf', 'turnstile', 'captcha', 'recaptcha', 
            'hcaptcha', 'ray id', 'cf-ray', 'cdn-cgi', 'data-cf-beacon'
        )
        
        # Check for strong indicators first
        strong_matches = sum(1 for k in strong_indicators if k in low)
        if strong_matches >= 1:
            return True
            
        # For weak indicators, need multiple matches AND small HTML
        weak_matches = sum(1 for k in weak_indicators if k in low)
        if weak_matches >= 3 and len(html) < 5000:
            return True
            
        # Special case: "cloudflare" mention with very small HTML
        if 'cloudflare' in low and len(html) < 2000:
            return True
            
        return False

    def _is_cloudflare_wait_page(self, url: str, timeout: int = 5) -> bool:
        """
        Lightweight preflight to detect Cloudflare wait/challenge without invoking the dynamic renderer.
        Heuristics:
        - Response headers suggest Cloudflare (Server: cloudflare or any 'cf-' header), especially with 403/429/503
        - HTML snippet includes common CF challenge phrases or cdn-cgi assets
        """
        try:
            ua = (
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '
                'AppleWebKit/537.36 (KHTML, like Gecko) '
                'Chrome/120.0 Safari/537.36'
            )
            req = Request(
                url,
                headers={
                    'User-Agent': ua,
                    'Accept': 'text/html,application/xhtml+xml;q=0.9,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.9,id;q=0.8',
                    'Cache-Control': 'no-cache'
                }
            )
            resp = urlopen(req, timeout=max(1, int(timeout)))
            status = getattr(resp, 'status', None)
            headers = resp.headers or {}
            server = (headers.get('Server') or headers.get('server') or '').lower()
            # Any header key starting with cf- is a strong signal
            has_cf_header = False
            try:
                for k in headers.keys():
                    if str(k).lower().startswith('cf-'):
                        has_cf_header = True
                        break
            except Exception:
                pass
            content_type = (headers.get('Content-Type') or '').lower()

            # Header-based detection
            if server.find('cloudflare') != -1 or has_cf_header:
                if status in (403, 429, 503):
                    return True

            if ('text/html' not in content_type) and ('application/xhtml+xml' not in content_type):
                return False

            raw = resp.read(4096)
            try:
                snippet = raw.decode('utf-8', errors='ignore')
            except Exception:
                snippet = raw.decode('latin-1', errors='ignore')

            final_url = getattr(resp, 'geturl', lambda: url)().lower()
            if self._looks_like_challenge(snippet, final_url):
                return True

            # Weak-signal fallback: Cloudflare server and tiny HTML without a normal body
            if (server.find('cloudflare') != -1 or has_cf_header):
                low = snippet.lower()
                if (len(snippet) < 2048) and ('<body' not in low or 'content="0;url=' in low):
                    return True

            return False
        except Exception:
            return False

    def _try_static_fetch(self, url: str) -> Optional[Dict]:
        """
        Perform a fast static HTTP GET and return a scrape-like result if suitable.

        Returns None if content is not HTML, looks like a challenge, or errors occur.
        """
        start_time = time.time()
        try:
            ua = (
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '
                'AppleWebKit/537.36 (KHTML, like Gecko) '
                'Chrome/120.0 Safari/537.36'
            )
            req = Request(
                url,
                headers={
                    'User-Agent': ua,
                    'Accept': 'text/html,application/xhtml+xml;q=0.9,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.9,id;q=0.8'
                }
            )
            resp = urlopen(req, timeout=min(self.timeout, 12))
            content_type = (resp.headers.get('Content-Type') or '').lower()
            if ('text/html' not in content_type) and ('application/xhtml+xml' not in content_type):
                return None
            raw = resp.read()
            try:
                html = raw.decode('utf-8', errors='replace')
            except Exception:
                html = raw.decode('latin-1', errors='replace')
            final_url = getattr(resp, 'geturl', lambda: url)()

            # Heuristic: skip if content looks like a challenge or is too short
            if self._looks_like_challenge(html, final_url) or len(html) < 400:
                if self.skip_on_challenge:
                    return {
                        'status': 503,
                        'html': '',
                        'url': url,
                        'final_url': final_url,
                        'error': 'cloudflare_challenge_detected_skip_static',
                        'load_time': time.time() - start_time,
                        'page_title': '',
                        'meta_description': '',
                        'is_contact_page': False
                    }
                return None

            result = {
                'status': getattr(resp, 'status', 200) or 200,
                'html': html,
                'url': url,
                'final_url': final_url,
                'error': None,
                'load_time': time.time() - start_time,
                'page_title': '',
                'meta_description': '',
                'is_contact_page': False
            }

            if html:
                soup = BeautifulSoup(html, 'html.parser')
                title_tag = soup.find('title')
                if title_tag:
                    result['page_title'] = title_tag.get_text().strip()
                meta_desc = soup.find('meta', attrs={'name': 'description'})
                if meta_desc:
                    result['meta_description'] = meta_desc.get('content', '').strip()
                result['is_contact_page'] = self.is_contact_page(html, final_url)

            return result
        except (HTTPError, URLError) as e:
            # Network or HTTP error on static path -> defer to dynamic
            return None
        except Exception:
            return None

    def _fetch_with_timeout(self, url: str, timeout: int, google_search: bool, network_idle: bool):
        """
        Perform StealthyFetcher.fetch in an isolated subprocess with a hard wall-clock timeout.
        Returns a SimpleNamespace page-like object on success, or None on timeout/error.
        This prevents indefinite blocking on Cloudflare wait pages and silences child logs.
        """
        # Get proxy configuration if available
        proxy_config = None
        if self.proxy_manager.has_proxies():
            proxy_info = self.proxy_manager.get_next_proxy()
            if proxy_info:
                proxy_config = self.proxy_manager.convert_to_playwright_format(proxy_info)
        
        result_q = multiprocessing.Queue()
        proc = multiprocessing.Process(
            target=_subprocess_fetch,
            args=(
                result_q,
                url,
                self.headless,
                self.solve_cloudflare,
                network_idle,
                google_search,
                timeout,
                self.block_images,
                self.disable_resources,
                proxy_config
            ),
            daemon=True
        )
        try:
            proc.start()
        except Exception:
            # Fallback to immediate failure if process cannot start
            return None

        msg = None
        try:
            msg = result_q.get(timeout=max(1, int(timeout)))
        except Exception:
            msg = None

        if msg is None:
            # Hard timeout: terminate child process to stop any ongoing logging loops
            try:
                proc.terminate()
            except Exception:
                pass
            try:
                proc.join(timeout=3)
            except Exception:
                pass
            # Keep silent on timeout to avoid spam
            return None

        # Ensure child exits
        try:
            proc.join(timeout=3)
        except Exception:
            pass

        if msg.get('ok'):
            return SimpleNamespace(
                status=msg.get('status', 200),
                html_content=msg.get('html_content', ''),
                url=msg.get('final_url', url),
                proxy_used=proxy_config.get('server') if proxy_config else None
            )
        else:
            # Mark proxy as failed if error occurred
            if proxy_config and 'server' in proxy_config:
                error_msg = msg.get('error', '')
                # Check if error is proxy-related
                if any(keyword in error_msg.lower() for keyword in ['proxy', 'connection', 'timeout', 'refused']):
                    self.proxy_manager.mark_proxy_failed(proxy_config['server'])

        return None

    def scrape_url(self, url: str) -> Dict:
        """
        Fetch webpage content with anti-bot bypass and JavaScript rendering.
        Enhanced with better error handling and contact page detection.

        Args:
            url (str): Target URL to scrape

        Returns:
            Dict: Scraping result with status, HTML content, and metadata
        """
        result = {
            'status': 0,
            'html': '',
            'url': url,
            'final_url': url,
            'error': None,
            'load_time': 0,
            'page_title': '',
            'meta_description': '',
            'is_contact_page': False
        }

        start_time = time.time()

        # Set domain context for Scrapling logs and emit a domain line up-front
        _SCRAPLING_CONTEXT.domain = (urlparse(url).netloc or '').lower()
        # Intentionally do not emit via the 'scrapling' logger to avoid triggering its handlers
        # and potential noisy output from third-party logging configuration.
        # No-op here.

        try:
            # Preflight: if a Cloudflare wait page is detected, poll up to cf_wait_timeout
            # without invoking the dynamic renderer, then fail fast. This prevents the internal
            # StealthyFetcher from hanging on the wait page and keeps changes scoped to CF-only paths.
            if self.solve_cloudflare:
                try:
                    if self._is_cloudflare_wait_page(url, timeout=3):
                        try:
                            logging.getLogger('scrapling').info("Cloudflare wait page detected (preflight); polling until timeout")
                        except Exception:
                            pass
                        deadline = start_time + self.cf_wait_timeout
                        backoff = 1
                        while time.time() < deadline:
                            time.sleep(backoff)
                            # Re-check if the wait page has cleared
                            if not self._is_cloudflare_wait_page(url, timeout=3):
                                break
                            backoff = min(backoff * 2, 6)
                        # Still a wait page after polling window -> fail fast without dynamic fetcher
                        if self._is_cloudflare_wait_page(url, timeout=3):
                            result['error'] = f"Cloudflare wait page persisted after {self.cf_wait_timeout}s; skipping"
                            result['status'] = 408
                            result['load_time'] = time.time() - start_time
                            return result
                except Exception:
                    # Ignore preflight errors and proceed
                    pass

            # 1) Static-first: attempt lightweight HTML fetch, then fall back
            if self.static_first:
                static_result = self._try_static_fetch(url)
                if static_result is not None:
                    return static_result

            # 2) Fall back to StealthyFetcher with rendering/anti-bot bypass
            # Enhanced: Use longer timeout for initial fetch to allow challenge completion
            # Previous 5s was too short for complex challenges
            eff_timeout = (self.cf_wait_timeout if self.solve_cloudflare else self.timeout)
            page = self._fetch_with_timeout(
                url,
                timeout=eff_timeout,
                google_search=False,
                network_idle=False,  # Always False for CF to prevent infinite waiting
            )
            if page is None:
                # Initial dynamic attempt timed out; annotate and proceed to CF handling/backoff
                result['status'] = 408 if self.solve_cloudflare else (result['status'] or 500)
                result['error'] = result['error'] or 'dynamic_fetch_timeout'
                result['final_url'] = url
                result['html'] = ''
                result['load_time'] = time.time() - start_time
            else:
                result['status'] = getattr(page, 'status', 200)
                result['html'] = getattr(page, 'html_content', '')
                result['final_url'] = getattr(page, 'url', url)
                result['load_time'] = time.time() - start_time

            # Detect Cloudflare challenge and apply bounded retry/backoff
            if self.solve_cloudflare:
                if (not result['html']) or self._looks_like_challenge(result['html'], result['final_url']):
                    # Optional fast-fail: skip immediately on detected challenge
                    if self.skip_on_challenge:
                        result['error'] = 'cloudflare_challenge_detected_skip_dynamic'
                        # Use 503 to indicate service unavailable due to challenge
                        if not isinstance(result['status'], int) or result['status'] in (200, 0):
                            result['status'] = 503
                        return result
                    try:
                        logging.getLogger('scrapling').info("Cloudflare challenge detected; applying bounded wait/backoff")
                    except Exception:
                        pass
                    deadline = start_time + self.cf_wait_timeout
                    backoff = 1
                    while time.time() < deadline:
                        remaining = max(1, int(deadline - time.time()))
                        try:
                            time.sleep(backoff)
                            page = self._fetch_with_timeout(
                                url,
                                timeout=min(remaining, 30),  # Increased from 10 to 30 seconds
                                google_search=True,
                                network_idle=True,  # Enable network idle for challenge solving
                            )
                            if page is not None:
                                result['status'] = getattr(page, 'status', 200)
                                result['html'] = getattr(page, 'html_content', '')
                                result['final_url'] = getattr(page, 'url', url)
                                result['load_time'] = time.time() - start_time
                                # Exit loop once challenge no longer detected
                                if result['html'] and not self._looks_like_challenge(result['html'], result['final_url']):
                                    break
                            # When page is None, attempt timed out; continue polling/backoff until deadline
                        except Exception:
                            # Break on fetch errors to avoid indefinite loops
                            break
                        backoff = min(backoff * 2, 8)

                    # If still challenge after bounded wait, annotate error and fail fast
                    if (not result['html']) or self._looks_like_challenge(result['html'], result['final_url']):
                        result['error'] = f"Cloudflare challenge persisted after {self.cf_wait_timeout}s; skipping"
                        if not isinstance(result['status'], int) or result['status'] in (200, 0):
                            result['status'] = 408

            # Extract additional metadata (based on final result HTML)
            if result['html']:
                soup = BeautifulSoup(result['html'], 'html.parser')

                title_tag = soup.find('title')
                if title_tag:
                    result['page_title'] = title_tag.get_text().strip()

                meta_desc = soup.find('meta', attrs={'name': 'description'})
                if meta_desc:
                    result['meta_description'] = meta_desc.get('content', '').strip()

                result['is_contact_page'] = self.is_contact_page(result['html'], result['final_url'])

        except Exception as e:
            elapsed = time.time() - start_time
            # If Cloudflare wait exceeded per-URL threshold, mark and skip
            if self.solve_cloudflare and elapsed >= self.cf_wait_timeout:
                result['error'] = f"Cloudflare wait page timeout exceeded {self.cf_wait_timeout}s; skipping"
                result['status'] = 408
            else:
                result['error'] = str(e)
                # Set status to indicate error if not already set
                if result['status'] == 0:
                    result['status'] = 500
            result['load_time'] = elapsed

        finally:
            # Clear domain context to avoid leaking to other requests/threads
            try:
                _SCRAPLING_CONTEXT.domain = None
            except Exception:
                pass

        return result

    def gather_contact_info(self, url: str) -> Dict:
        """
        Comprehensive contact information gathering inspired by sampler.txt.
        Scrapes main page and contact pages for email and phone information.

        Args:
            url (str): Target URL to scrape

        Returns:
            Dict: Comprehensive contact information
        """
        result = {
            'website': url,
            'emails': [],
            'phones': [],
            'whatsapp': [],
            'pages_scraped': [],
            'status': 'failed',
            'error': None
        }

        try:
            # First, scrape the main page
            main_result = self.scrape_url(url)

            if main_result.get('error'):
                result['error'] = main_result['error']
                return result

            if not main_result.get('html'):
                result['error'] = 'No HTML content retrieved'
                return result

            # Extract contacts prioritizing header > footer > structured data > general page
            from contact_extractor import ContactExtractor
            extractor = ContactExtractor()

            soup = BeautifulSoup(main_result['html'], 'html.parser')

            # 1) Header-first scan
            header_selectors = 'header, #header, .header, .site-header, .main-header, nav, #nav, .navbar, .navigation'
            for section in soup.select(header_selectors):
                contacts = extractor.extract_all_contacts(str(section), main_result['final_url'])
                for contact in contacts:
                    field = contact.get('field')
                    value = contact.get('value_normalized') or contact.get('value_raw')
                    if not value:
                        continue
                    if field == 'email':
                        result['emails'].append(value)
                    elif field == 'phone':
                        result['phones'].append(value)
                    elif field == 'whatsapp':
                        result['whatsapp'].append(value)

            # 2) Footer scan
            footer_selectors = 'footer, #footer, .footer, .site-footer, .main-footer'
            for section in soup.select(footer_selectors):
                contacts = extractor.extract_all_contacts(str(section), main_result['final_url'])
                for contact in contacts:
                    field = contact.get('field')
                    value = contact.get('value_normalized') or contact.get('value_raw')
                    if not value:
                        continue
                    if field == 'email':
                        result['emails'].append(value)
                    elif field == 'phone':
                        result['phones'].append(value)
                    elif field == 'whatsapp':
                        result['whatsapp'].append(value)

            # 3) Structured data (JSON-LD) scan
            structured = self.extract_structured_data(main_result['html'])
            for em in structured.get('emails', []):
                result['emails'].append(em)
            for ph in structured.get('phones', []):
                result['phones'].append(ph)

            # 4) General page scan as fallback
            main_contacts = extractor.extract_all_contacts(main_result['html'])
            for contact in main_contacts:
                field = contact.get('field')
                value = contact.get('value_normalized') or contact.get('value_raw')
                if not value:
                    continue
                if field == 'email':
                    result['emails'].append(value)
                elif field == 'phone':
                    result['phones'].append(value)
                elif field == 'whatsapp':
                    result['whatsapp'].append(value)
            result['pages_scraped'].append({
                'url': main_result['final_url'],
                'title': main_result.get('page_title', ''),
                'is_contact_page': main_result.get('is_contact_page', False)
            })

            # Look for contact page links
            soup = BeautifulSoup(main_result['html'], 'html.parser')
            contact_links = self._find_contact_links(soup, main_result['final_url'], self._get_priority_paths())

            # Scrape contact pages for additional information
            for contact_url in contact_links[:3]:  # Limit to 3 contact pages
                contact_result = self.scrape_url(contact_url)

                if contact_result.get('html') and not contact_result.get('error'):
                    # Structured data first on contact page
                    structured_cp = self.extract_structured_data(contact_result['html'])
                    for em in structured_cp.get('emails', []):
                        result['emails'].append(em)
                    for ph in structured_cp.get('phones', []):
                        result['phones'].append(ph)

                    # Then header/footer sections on contact page
                    cp_soup = BeautifulSoup(contact_result['html'], 'html.parser')
                    for section in cp_soup.select(header_selectors):
                        contacts = extractor.extract_all_contacts(str(section), contact_result['final_url'])
                        for contact in contacts:
                            field = contact.get('field')
                            value = contact.get('value_normalized') or contact.get('value_raw')
                            if not value:
                                continue
                            if field == 'email':
                                result['emails'].append(value)
                            elif field == 'phone':
                                result['phones'].append(value)
                            elif field == 'whatsapp':
                                result['whatsapp'].append(value)

                    for section in cp_soup.select(footer_selectors):
                        contacts = extractor.extract_all_contacts(str(section), contact_result['final_url'])
                        for contact in contacts:
                            field = contact.get('field')
                            value = contact.get('value_normalized') or contact.get('value_raw')
                            if not value:
                                continue
                            if field == 'email':
                                result['emails'].append(value)
                            elif field == 'phone':
                                result['phones'].append(value)
                            elif field == 'whatsapp':
                                result['whatsapp'].append(value)

                    # Finally, general page scan
                    contact_contacts = extractor.extract_all_contacts(contact_result['html'])
                    for contact in contact_contacts:
                        field = contact.get('field')
                        value = contact.get('value_normalized') or contact.get('value_raw')
                        if not value:
                            continue
                        if field == 'email':
                            result['emails'].append(value)
                        elif field == 'phone':
                            result['phones'].append(value)
                        elif field == 'whatsapp':
                            result['whatsapp'].append(value)
                    result['pages_scraped'].append({
                        'url': contact_result['final_url'],
                        'title': contact_result.get('page_title', ''),
                        'is_contact_page': contact_result.get('is_contact_page', True)
                    })

            # Remove duplicates
            # Final cleanup: normalize emails to strip accidental tokens then deduplicate
            try:
                cleaned_emails = []
                for em in result['emails']:
                    norm = extractor._normalize_email(em)
                    if norm:
                        cleaned_emails.append(norm)
                # Preserve order while deduplicating
                result['emails'] = list(dict.fromkeys(cleaned_emails))
            except Exception:
                result['emails'] = list(set(result['emails']))
            result['phones'] = list(set(result['phones']))
            result['whatsapp'] = list(set(result['whatsapp']))

            # Set status based on results
            if result['emails'] or result['phones'] or result['whatsapp']:
                result['status'] = 'success'
            else:
                result['status'] = 'no_contacts_found'

        except Exception as e:
            result['error'] = str(e)
            result['status'] = 'error'

        return result

    def _get_priority_paths(self) -> List[str]:
        """Get priority paths for contact page detection"""
        return [
            # English variants
            '/contact', '/contact-us', '/contacts', '/contactus', '/contact_us',
            '/about', '/about-us', '/aboutus', '/about_us', '/company',
            '/support', '/customer-service', '/customer_support', '/help',

            # Indonesian variants
            '/kontak', '/hubungi', '/hubungi-kami', '/tentang', '/tentang-kami',
            '/layanan', '/bantuan', '/dukungan', '/informasi',

            # Common patterns
            '/info', '/information', '/reach-us', '/get-in-touch'
        ]
        """
        Scrape multiple pages from a website, focusing on contact-related pages.

        Args:
            url (str): Base URL to start scraping
            max_pages (int): Maximum number of pages to scrape

        Returns:
            List[Dict]: List of scraping results for each page
        """
        results = []
        visited_urls = set()

        # Priority pages to look for
        # Enhanced priority pages with multi-language support
        priority_paths = [
            # English variants
            '/contact', '/contact-us', '/contacts', '/contactus', '/contact_us',
            '/about', '/about-us', '/aboutus', '/about_us', '/company',
            '/support', '/customer-service', '/customer_support', '/help', '/faq',
            '/sales', '/info', '/information', '/reach-us', '/get-in-touch',

            # Indonesian variants
            '/kontak', '/hubungi', '/hubungi-kami', '/tentang', '/tentang-kami',
            '/layanan', '/bantuan', '/dukungan', '/informasi', '/perusahaan',

            # Spanish variants
            '/contacto', '/contactanos', '/acerca', '/acerca-de', '/sobre-nosotros',
            '/soporte', '/ayuda', '/informacion', '/empresa', '/servicio-cliente',

            # French variants
            '/contact', '/contactez-nous', '/a-propos', '/apropos', '/sur-nous',
            '/support', '/aide', '/assistance', '/service-client', '/entreprise',

            # German variants
            '/kontakt', '/kontaktieren', '/uber-uns', '/ueber-uns', '/unternehmen',
            '/support', '/hilfe', '/kundenservice', '/kundendienst', '/info',

            # Portuguese variants
            '/contato', '/contatos', '/fale-conosco', '/sobre', '/sobre-nos',
            '/suporte', '/ajuda', '/atendimento', '/empresa', '/informacoes',

            # Italian variants
            '/contatto', '/contatti', '/contattaci', '/chi-siamo', '/azienda',
            '/supporto', '/aiuto', '/assistenza', '/servizio-clienti', '/info',

            # Dutch variants
            '/contact', '/contacteer', '/over-ons', '/bedrijf', '/ondersteuning',
            '/hulp', '/klantenservice', '/informatie', '/bereik-ons',

            # Russian variants (transliterated)
            '/kontakt', '/kontakty', '/o-nas', '/o-kompanii', '/podderzhka',
            '/pomosh', '/informatsiya', '/svyazatsya', '/obsluzhivanie',

            # Chinese (Pinyin)
            '/lianxi', '/guanyu', '/bangzhu', '/kefu', '/fuwu', '/xinxi',

            # Japanese (Romaji)
            '/otoiawase', '/kaisha', '/kaisya', '/support', '/help', '/info',

            # Arabic (transliterated)
            '/ittasal', '/hawl', '/daem', '/musaada', '/malumat', '/sharikat',

            # Common patterns across languages
            '/team', '/staff', '/office', '/location', '/address', '/phone',
            '/email', '/form', '/inquiry', '/enquiry', '/feedback', '/message',
            '/reach', '/connect', '/communicate', '/talk', '/chat', '/call',

            # Business-specific paths
            '/sales-team', '/business', '/partnership', '/investor', '/media',
            '/press', '/careers', '/jobs', '/hr', '/recruitment', '/legal',
            '/privacy', '/terms', '/policy', '/compliance', '/security',

            # Regional/Country specific
            '/us', '/usa', '/uk', '/eu', '/asia', '/global', '/international',
            '/local', '/regional', '/branch', '/subsidiary', '/affiliate'
        ]

        # Start with the main page
        main_result = self.scrape_url(url)
        results.append(main_result)
        visited_urls.add(url)

        if main_result['status'] != 200 or not main_result['html']:
            return results

        # Parse the main page to find contact-related links
        soup = BeautifulSoup(main_result['html'], 'html.parser')
        base_url = self._get_base_url(main_result['final_url'])

        # Find potential contact pages
        contact_links = self._find_contact_links(soup, base_url, priority_paths)

        # Scrape additional pages up to max_pages limit
        scraped_count = 1
        for link_url in contact_links:
            if scraped_count >= max_pages:
                break

            if link_url not in visited_urls:
                page_result = self.scrape_url(link_url)
                results.append(page_result)
                visited_urls.add(link_url)
                scraped_count += 1

                # Small delay between requests to be respectful
                time.sleep(1)

        return results

    def _get_base_url(self, url: str) -> str:
        """Get base URL from a full URL."""
        parsed = urlparse(url)
        return f"{parsed.scheme}://{parsed.netloc}"

    def _find_contact_links(self, soup: BeautifulSoup, base_url: str, priority_paths: List[str]) -> List[str]:
        """
        Find contact-related links on a page.

        Args:
            soup (BeautifulSoup): Parsed HTML
            base_url (str): Base URL for resolving relative links
            priority_paths (List[str]): Priority paths to look for

        Returns:
            List[str]: List of contact-related URLs
        """
        contact_links = []
        found_paths = set()

        # Find all links
        all_links = soup.find_all('a', href=True)

        # Enhanced contact-related keywords with multi-language support
        contact_keywords = [
            # English keywords
            'contact', 'about', 'support', 'customer', 'sales', 'info', 'help',
            'cs', 'service', 'team', 'staff', 'office', 'reach', 'connect',
            'get in touch', 'talk to us', 'call us', 'email us', 'message',
            'inquiry', 'enquiry', 'feedback', 'company', 'business', 'corporate',

            # Indonesian keywords
            'kontak', 'hubungi', 'tentang', 'layanan', 'bantuan', 'dukungan',
            'informasi', 'perusahaan', 'tim', 'kantor', 'alamat', 'telepon',
            'email', 'formulir', 'pertanyaan', 'masukan', 'saran',

            # Spanish keywords
            'contacto', 'acerca', 'sobre', 'soporte', 'ayuda', 'servicio',
            'empresa', 'equipo', 'oficina', 'direccion', 'telefono',
            'correo', 'formulario', 'consulta', 'informacion',

            # French keywords
            'contact', 'propos', 'support', 'aide', 'service', 'entreprise',
            'equipe', 'bureau', 'adresse', 'telephone', 'courriel',
            'formulaire', 'demande', 'information', 'assistance',

            # German keywords
            'kontakt', 'uber', 'support', 'hilfe', 'service', 'unternehmen',
            'team', 'buro', 'adresse', 'telefon', 'email', 'formular',
            'anfrage', 'information', 'kundendienst',

            # Portuguese keywords
            'contato', 'sobre', 'suporte', 'ajuda', 'atendimento', 'empresa',
            'equipe', 'escritorio', 'endereco', 'telefone', 'email',
            'formulario', 'consulta', 'informacao', 'servico',

            # Italian keywords
            'contatto', 'riguardo', 'supporto', 'aiuto', 'servizio', 'azienda',
            'team', 'ufficio', 'indirizzo', 'telefono', 'email', 'modulo',
            'richiesta', 'informazione', 'assistenza',

            # Dutch keywords
            'contact', 'over', 'ondersteuning', 'hulp', 'service', 'bedrijf',
            'team', 'kantoor', 'adres', 'telefoon', 'email', 'formulier',
            'vraag', 'informatie', 'klantenservice',

            # Universal/Common keywords
            'phone', 'tel', 'fax', 'address', 'location', 'map', 'directions',
            'hours', 'schedule', 'appointment', 'booking', 'reservation',
            'quote', 'estimate', 'demo', 'trial', 'consultation', 'meeting'
        ]

        for link in all_links:
            href = link.get('href', '').strip()
            link_text = link.get_text().strip().lower()

            if not href:
                continue

            # Convert relative URLs to absolute
            full_url = urljoin(base_url, href)

            # Skip external links, javascript, mailto, tel links
            if not full_url.startswith(base_url) or \
               href.startswith(('javascript:', 'mailto:', 'tel:')):
                continue

            # Check if it's a priority path
            parsed_url = urlparse(full_url)
            path = parsed_url.path.lower()

            # Check priority paths first
            for priority_path in priority_paths:
                if priority_path in path and path not in found_paths:
                    contact_links.append(full_url)
                    found_paths.add(path)
                    break
            else:
                # Check link text for contact keywords
                for keyword in contact_keywords:
                    if keyword in link_text and path not in found_paths:
                        contact_links.append(full_url)
                        found_paths.add(path)
                        break

        return contact_links[:5]  # Limit to top 5 contact links

    def extract_structured_data(self, html: str) -> Dict:
        """
        Extract structured data (JSON-LD, microdata) from HTML.

        Args:
            html (str): HTML content

        Returns:
            Dict: Extracted structured data
        """
        structured_data = {
            'json_ld': [],
            'organization': {},
            'contact_points': [],
            'emails': [],
            'phones': []
        }

        if not html:
            return structured_data

        soup = BeautifulSoup(html, 'html.parser')
        # Use ContactExtractor for robust email matching in targeted fields
        try:
            from contact_extractor import ContactExtractor
            extractor = ContactExtractor()
        except Exception:
            extractor = None

        # Helper to normalize email strings robustly
        def _clean_email_str(s: str) -> Optional[str]:
            if not s:
                return None
            try:
                if extractor:
                    return extractor._normalize_email(s)
            except Exception:
                pass
            # Fallback strict pattern
            m = re.search(r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}', str(s).lower())
            return m.group(0) if m else None

        def collect_from_json_ld(obj):
            """Collect emails and phones from common JSON-LD structures recursively."""
            if isinstance(obj, dict):
                atype = obj.get('@type') or obj.get('type')
                # Organization, Person, LocalBusiness
                if atype in ('Organization', 'Person', 'LocalBusiness'):
                    email = obj.get('email')
                    telephone = obj.get('telephone')
                    if isinstance(email, str):
                        cleaned = _clean_email_str(email)
                        if cleaned:
                            structured_data['emails'].append(cleaned)
                    elif isinstance(email, list):
                        for e in email:
                            if isinstance(e, str):
                                cleaned = _clean_email_str(e)
                                if cleaned:
                                    structured_data['emails'].append(cleaned)
                    if isinstance(telephone, str):
                        structured_data['phones'].append(telephone)
                    elif isinstance(telephone, list):
                        structured_data['phones'].extend([t for t in telephone if isinstance(t, str)])

                    # ContactPoint(s)
                    contact_point = obj.get('contactPoint') or obj.get('contactPoints')
                    if isinstance(contact_point, dict):
                        cp_email = contact_point.get('email')
                        cp_tel = contact_point.get('telephone')
                        if isinstance(cp_email, str):
                            cleaned = _clean_email_str(cp_email)
                            if cleaned:
                                structured_data['emails'].append(cleaned)
                        elif isinstance(cp_email, list):
                            for e in cp_email:
                                if isinstance(e, str):
                                    cleaned = _clean_email_str(e)
                                    if cleaned:
                                        structured_data['emails'].append(cleaned)
                        if isinstance(cp_tel, str):
                            structured_data['phones'].append(cp_tel)
                        elif isinstance(cp_tel, list):
                            structured_data['phones'].extend([t for t in cp_tel if isinstance(t, str)])
                    elif isinstance(contact_point, list):
                        for cp in contact_point:
                            if isinstance(cp, dict):
                                cp_email = cp.get('email')
                                cp_tel = cp.get('telephone')
                                if isinstance(cp_email, str):
                                    cleaned = _clean_email_str(cp_email)
                                    if cleaned:
                                        structured_data['emails'].append(cleaned)
                                elif isinstance(cp_email, list):
                                    for e in cp_email:
                                        if isinstance(e, str):
                                            cleaned = _clean_email_str(e)
                                            if cleaned:
                                                structured_data['emails'].append(cleaned)
                                if isinstance(cp_tel, str):
                                    structured_data['phones'].append(cp_tel)
                                elif isinstance(cp_tel, list):
                                    structured_data['phones'].extend([t for t in cp_tel if isinstance(t, str)])

                # Generic keys fallback
                for k, v in obj.items():
                    if k.lower() == 'email':
                        if isinstance(v, str):
                            cleaned = _clean_email_str(v)
                            if cleaned:
                                structured_data['emails'].append(cleaned)
                        elif isinstance(v, list):
                            for e in v:
                                if isinstance(e, str):
                                    cleaned = _clean_email_str(e)
                                    if cleaned:
                                        structured_data['emails'].append(cleaned)
                    if k.lower() in ('telephone', 'phone'):
                        if isinstance(v, str):
                            structured_data['phones'].append(v)
                        elif isinstance(v, list):
                            structured_data['phones'].extend([t for t in v if isinstance(t, str)])
                    if isinstance(v, (dict, list)):
                        collect_from_json_ld(v)
            elif isinstance(obj, list):
                for item in obj:
                    collect_from_json_ld(item)

        # Extract JSON-LD data
        json_ld_scripts = soup.find_all('script', type='application/ld+json')
        for script in json_ld_scripts:
            try:
                data = json.loads(script.string)
                structured_data['json_ld'].append(data)

                # Extract organization data
                if isinstance(data, dict):
                    if data.get('@type') == 'Organization':
                        structured_data['organization'] = data
                    elif data.get('@type') == 'ContactPage':
                        structured_data['contact_points'].append(data)
                elif isinstance(data, list):
                    for item in data:
                        if isinstance(item, dict):
                            if item.get('@type') == 'Organization':
                                structured_data['organization'] = item
                            elif item.get('@type') == 'ContactPage':
                                structured_data['contact_points'].append(item)
                # Collect emails/phones from data
                collect_from_json_ld(data)
            except (json.JSONDecodeError, AttributeError):
                continue

        # Microdata: scan itemprop attributes for email/telephone
        email_props = {
            'email', 'emailaddress', 'contactemail', 'e-mail', 'mail', 'email_address'
        }
        phone_props = {
            'telephone', 'phone', 'phonenumber', 'contactphone', 'contactnumber', 'tel', 'phone_number'
        }

        for tag in soup.find_all(attrs={'itemprop': True}):
            prop = (tag.get('itemprop') or '').strip().lower()
            if not prop:
                continue
            candidates = []
            content_val = tag.get('content')
            href_val = tag.get('href')
            text_val = tag.get_text(' ', strip=True)
            if content_val:
                candidates.append(content_val)
            if href_val:
                candidates.append(href_val)
            if text_val:
                candidates.append(text_val)
            if prop in email_props:
                for c in candidates:
                    if not c:
                        continue
                    if 'mailto:' in c:
                        email = c.split('mailto:', 1)[1].split('?')[0].strip()
                        cleaned = _clean_email_str(email)
                        if cleaned:
                            structured_data['emails'].append(cleaned)
                    else:
                        if extractor:
                            try:
                                for e in extractor.extract_emails(c):
                                    val = e.get('value_normalized') or e.get('value_raw')
                                    cleaned = _clean_email_str(val)
                                    if cleaned:
                                        structured_data['emails'].append(cleaned)
                            except Exception:
                                pass
            if prop in phone_props:
                for c in candidates:
                    if not c:
                        continue
                    if 'tel:' in c:
                        phone = c.split('tel:', 1)[1].split('?')[0].strip()
                        if phone:
                            structured_data['phones'].append(phone)
                    else:
                        # Naive phone capture from targeted microdata value
                        digits = ''.join(ch for ch in c if ch.isdigit() or ch in '+() -')
                        if sum(ch.isdigit() for ch in digits) >= 6:
                            structured_data['phones'].append(digits.strip())

        # RDFa: scan property/typeof for common email/telephone predicates
        for tag in soup.find_all(attrs={'property': True}):
            prop = (tag.get('property') or '').strip().lower()
            candidates = []
            for attr in ('content', 'href', 'resource'):
                val = tag.get(attr)
                if val:
                    candidates.append(val)
            text_val = tag.get_text(' ', strip=True)
            if text_val:
                candidates.append(text_val)

            if prop in ('email', 'schema:email', 'vcard:email', 'foaf:mbox', 'contactemail'):
                for c in candidates:
                    if not c:
                        continue
                    if 'mailto:' in c:
                        email = c.split('mailto:', 1)[1].split('?')[0].strip()
                        cleaned = _clean_email_str(email)
                        if cleaned:
                            structured_data['emails'].append(cleaned)
                    else:
                        if extractor:
                            try:
                                for e in extractor.extract_emails(c):
                                    val = e.get('value_normalized') or e.get('value_raw')
                                    cleaned = _clean_email_str(val)
                                    if cleaned:
                                        structured_data['emails'].append(cleaned)
                            except Exception:
                                pass

            if prop in ('telephone', 'phone', 'schema:telephone', 'vcard:tel', 'contactphone'):
                for c in candidates:
                    if not c:
                        continue
                    if 'tel:' in c:
                        phone = c.split('tel:', 1)[1].split('?')[0].strip()
                        if phone:
                            structured_data['phones'].append(phone)
                    else:
                        digits = ''.join(ch for ch in c if ch.isdigit() or ch in '+() -')
                        if sum(ch.isdigit() for ch in digits) >= 6:
                            structured_data['phones'].append(digits.strip())

        # Link rel: extract mailto or email-like hrefs
        for link in soup.find_all('link', href=True):
            rels = link.get('rel') or []
            rels_list = rels if isinstance(rels, list) else [rels]
            rels_norm = [str(r).lower() for r in rels_list]
            href = link.get('href', '').strip()
            if not href:
                continue
            if 'mailto:' in href or any(r in ('me', 'author', 'contact', 'reply-to', 'email') for r in rels_norm):
                if 'mailto:' in href:
                    email = href.split('mailto:', 1)[1].split('?')[0].strip()
                    cleaned = _clean_email_str(email)
                    if cleaned:
                        structured_data['emails'].append(cleaned)
                else:
                    if extractor:
                        try:
                            for e in extractor.extract_emails(href):
                                val = e.get('value_normalized') or e.get('value_raw')
                                cleaned = _clean_email_str(val)
                                if cleaned:
                                    structured_data['emails'].append(cleaned)
                        except Exception:
                            pass

        # Meta tags: scan content for email hints
        for meta in soup.find_all('meta'):
            content = meta.get('content')
            name = (meta.get('name') or meta.get('property') or '').lower()
            if not content:
                continue
            if any(k in name for k in ('email', 'contact', 'reply-to', 'author', 'publisher', 'creator')):
                if extractor:
                    try:
                        for e in extractor.extract_emails(content):
                            val = e.get('value_normalized') or e.get('value_raw')
                            cleaned = _clean_email_str(val)
                            if cleaned:
                                structured_data['emails'].append(cleaned)
                    except Exception:
                        pass
            else:
                if 'mailto:' in content:
                    email = content.split('mailto:', 1)[1].split('?')[0].strip()
                    cleaned = _clean_email_str(email)
                    if cleaned:
                        structured_data['emails'].append(cleaned)
                else:
                    if extractor:
                        try:
                            for e in extractor.extract_emails(content):
                                val = e.get('value_normalized') or e.get('value_raw')
                                cleaned = _clean_email_str(val)
                                if cleaned:
                                    structured_data['emails'].append(cleaned)
                        except Exception:
                            pass

        # Deduplicate collected structured emails/phones
        structured_data['emails'] = list({e.strip().lower() for e in structured_data['emails'] if isinstance(e, str) and e.strip()})
        structured_data['phones'] = list({p.strip() for p in structured_data['phones'] if isinstance(p, str) and p.strip()})

        return structured_data

    def is_contact_page(self, html: str, url: str) -> bool:
        """
        Determine if a page is likely a contact page.

        Args:
            html (str): HTML content
            url (str): Page URL

        Returns:
            bool: True if likely a contact page
        """
        if not html:
            return False

        # Check URL path
        url_lower = url.lower()
        # Enhanced multi-language contact URL keywords
        contact_url_keywords = [
            # English
            'contact', 'about', 'support', 'help', 'info', 'team', 'company',
            'reach', 'connect', 'touch', 'call', 'phone', 'email', 'message',

            # Indonesian
            'kontak', 'hubungi', 'tentang', 'layanan', 'bantuan', 'dukungan',
            'informasi', 'perusahaan', 'tim', 'kantor', 'alamat', 'telepon',

            # Spanish
            'contacto', 'acerca', 'sobre', 'soporte', 'ayuda', 'servicio',
            'empresa', 'equipo', 'oficina', 'direccion', 'telefono', 'correo',

            # French
            'contact', 'propos', 'support', 'aide', 'service', 'entreprise',
            'equipe', 'bureau', 'adresse', 'telephone', 'courriel',

            # German
            'kontakt', 'uber', 'hilfe', 'service', 'unternehmen', 'team',
            'buro', 'adresse', 'telefon', 'email', 'kundendienst',

            # Portuguese
            'contato', 'sobre', 'suporte', 'ajuda', 'atendimento', 'empresa',
            'equipe', 'escritorio', 'endereco', 'telefone',

            # Italian
            'contatto', 'riguardo', 'supporto', 'aiuto', 'servizio', 'azienda',
            'team', 'ufficio', 'indirizzo', 'telefono', 'assistenza',

            # Dutch
            'contact', 'over', 'ondersteuning', 'hulp', 'service', 'bedrijf',
            'team', 'kantoor', 'adres', 'telefoon', 'klantenservice'
        ]

        for keyword in contact_url_keywords:
            if keyword in url_lower:
                return True

        # Check page content
        soup = BeautifulSoup(html, 'html.parser')

        # Check title
        title = soup.find('title')
        if title:
            title_text = title.get_text().lower()
            for keyword in contact_url_keywords:
                if keyword in title_text:
                    return True

        # Check headings
        headings = soup.find_all(['h1', 'h2', 'h3'])
        for heading in headings:
            heading_text = heading.get_text().lower()
            for keyword in contact_url_keywords:
                if keyword in heading_text:
                    return True

        # Enhanced form field detection with multi-language support
        forms = soup.find_all('form')
        form_keywords = [
            # English
            'email', 'message', 'name', 'phone', 'subject', 'inquiry', 'comment',
            'feedback', 'question', 'request', 'contact', 'address', 'company',

            # Indonesian
            'email', 'pesan', 'nama', 'telepon', 'subjek', 'pertanyaan', 'komentar',
            'masukan', 'saran', 'permintaan', 'kontak', 'alamat', 'perusahaan',

            # Spanish
            'correo', 'mensaje', 'nombre', 'telefono', 'asunto', 'consulta', 'comentario',
            'pregunta', 'solicitud', 'contacto', 'direccion', 'empresa',

            # French
            'courriel', 'message', 'nom', 'telephone', 'sujet', 'demande', 'commentaire',
            'question', 'requete', 'contact', 'adresse', 'entreprise',

            # German
            'email', 'nachricht', 'name', 'telefon', 'betreff', 'anfrage', 'kommentar',
            'frage', 'antrag', 'kontakt', 'adresse', 'unternehmen',

            # Portuguese
            'email', 'mensagem', 'nome', 'telefone', 'assunto', 'consulta', 'comentario',
            'pergunta', 'solicitacao', 'contato', 'endereco', 'empresa',

            # Universal patterns
            'submit', 'send', 'enviar', 'kirim', 'envoyer', 'senden', 'inviare'
        ]

        for form in forms:
            form_text = form.get_text().lower()
            # Also check input field names and placeholders
            inputs = form.find_all(['input', 'textarea', 'select'])
            for input_field in inputs:
                field_name = (input_field.get('name', '') + ' ' +
                            input_field.get('placeholder', '') + ' ' +
                            input_field.get('id', '')).lower()
                form_text += ' ' + field_name

            if any(keyword in form_text for keyword in form_keywords):
                return True

        return False